{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HrhJSVyeULDP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.num_patches = (224 // patch_size) ** 2  # Assuming input image size is 224x224\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x is an image tensor of shape (batch_size, channels, height, width)\n",
        "        # Extract patches from the input image tensor using conv2d\n",
        "        x = self.proj(x)\n",
        "        # Reshape the extracted patches into a 2D tensor\n",
        "        # (batch_size, num_patches, embedding_dim)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        # Prepend the learnable class token to the patch embeddings\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        # Add positional embeddings to the patch and class embeddings\n",
        "        x = x + self.pos_embed\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the image and resize it to 224x224\n",
        "image = Image.open('/content/dog.jpeg').convert('RGB')\n",
        "resized_image = image.resize((224, 224))\n",
        "\n",
        "# Convert the resized image to a tensor\n",
        "tensor_image = transforms.ToTensor()(resized_image)\n",
        "\n",
        "# Add a batch dimension to the tensor\n",
        "batched_tensor_image = tensor_image.unsqueeze(0)\n",
        "\n",
        "# Initialize the PatchEmbedding module with patch_size=16, embed_dim=768\n",
        "patch_embedding = PatchEmbedding(patch_size=16, embed_dim=768)\n",
        "\n",
        "# Compute patch embeddings for the input image tensor\n",
        "patch_embeddings = patch_embedding.forward(batched_tensor_image)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(patch_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8QVu-NwUnPY",
        "outputId": "de5902e2-d461-4d04-86b4-f8249ba2e7ca"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Multi-head self-attention layer\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate)\n",
        "\n",
        "        # Feedforward neural network layer\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute multi-head self-attention\n",
        "        x = x.transpose(0, 1) # (num_patches, batch_size, embed_dim)\n",
        "\n",
        "        # Apply residual connection and layer normalization\n",
        "        x = self.norm(x + x)\n",
        "\n",
        "        # Apply self attn\n",
        "        x, _ = self.self_attention(x, x, x)\n",
        "        x = x.transpose(0, 1) # (batch_size, num_patches, embed_dim)\n",
        "\n",
        "        # Apply residual connection and layer normalization\n",
        "        x = self.norm(x + x)\n",
        "\n",
        "        # Apply feedforward neural network\n",
        "        x = self.feedforward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(embed_dim, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Compute the mean of the patch embeddings\n",
        "        x = x.mean(dim=1)\n",
        "        # Project the patch embeddings to the class logits\n",
        "        x = self.proj(x)\n",
        "        # Apply softmax activation to get class probabilities\n",
        "        x = nn.functional.softmax(x, dim=-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b2U6B_gtU0Ar"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder(num_layers=3, num_heads=12, embed_dim=768)\n",
        "classhead = ClassificationHead(embed_dim=768, num_classes=10)\n",
        "\n",
        "encoded = encoder.forward(patch_embeddings)\n",
        "final = classhead.forward(encoded)\n",
        "\n",
        "print(final.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWmk0oUNVco7",
        "outputId": "0a51acb9-ca10-4153-d3b9-ddda5a35699b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set the device to run the model on\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the transformation to be applied to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset and apply the transformation\n",
        "train_dataset = datasets.CIFAR10('data', train=True, download=False, transform=transform)\n",
        "\n",
        "# Set the batch size and create a data loader for the training set\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the Vision Transformer model\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, embed_dim=768, num_layers=3, num_heads=12, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, embed_dim)\n",
        "        self.transformer = TransformerEncoder(num_layers, embed_dim, num_heads)\n",
        "        self.classification_head = ClassificationHead(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding.forward(x)\n",
        "        x = self.transformer.forward(x)\n",
        "        x = self.classification_head.forward(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the Vision Transformer model\n",
        "model = VisionTransformer().to(device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the number of epochs to train for\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Loop over the batches in the training set\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move the data to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.forward(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training status every 10 batches\n",
        "        if (i + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "0sW4KDNfV2i4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e125ff21-f6ae-4850-f7eb-f2bc5ce245e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/782], Loss: 2.2916\n",
            "Epoch [1/10], Step [20/782], Loss: 2.2746\n",
            "Epoch [1/10], Step [30/782], Loss: 2.2419\n",
            "Epoch [1/10], Step [40/782], Loss: 2.2335\n",
            "Epoch [1/10], Step [50/782], Loss: 2.2548\n",
            "Epoch [1/10], Step [60/782], Loss: 2.2256\n",
            "Epoch [1/10], Step [70/782], Loss: 2.2465\n",
            "Epoch [1/10], Step [80/782], Loss: 2.2247\n",
            "Epoch [1/10], Step [90/782], Loss: 2.2296\n",
            "Epoch [1/10], Step [100/782], Loss: 2.2542\n",
            "Epoch [1/10], Step [110/782], Loss: 2.2397\n",
            "Epoch [1/10], Step [120/782], Loss: 2.2794\n",
            "Epoch [1/10], Step [130/782], Loss: 2.1918\n",
            "Epoch [1/10], Step [140/782], Loss: 2.2422\n",
            "Epoch [1/10], Step [150/782], Loss: 2.2834\n",
            "Epoch [1/10], Step [160/782], Loss: 2.2662\n",
            "Epoch [1/10], Step [170/782], Loss: 2.2454\n",
            "Epoch [1/10], Step [180/782], Loss: 2.2562\n",
            "Epoch [1/10], Step [190/782], Loss: 2.1463\n",
            "Epoch [1/10], Step [200/782], Loss: 2.1478\n",
            "Epoch [1/10], Step [210/782], Loss: 2.2507\n",
            "Epoch [1/10], Step [220/782], Loss: 2.2676\n",
            "Epoch [1/10], Step [230/782], Loss: 2.2475\n",
            "Epoch [1/10], Step [240/782], Loss: 2.2271\n",
            "Epoch [1/10], Step [250/782], Loss: 2.2867\n",
            "Epoch [1/10], Step [260/782], Loss: 2.2583\n",
            "Epoch [1/10], Step [270/782], Loss: 2.1908\n",
            "Epoch [1/10], Step [280/782], Loss: 2.2258\n",
            "Epoch [1/10], Step [290/782], Loss: 2.2432\n",
            "Epoch [1/10], Step [300/782], Loss: 2.1918\n",
            "Epoch [1/10], Step [310/782], Loss: 2.2304\n",
            "Epoch [1/10], Step [320/782], Loss: 2.2171\n",
            "Epoch [1/10], Step [330/782], Loss: 2.2621\n",
            "Epoch [1/10], Step [340/782], Loss: 2.2523\n",
            "Epoch [1/10], Step [350/782], Loss: 2.2141\n",
            "Epoch [1/10], Step [360/782], Loss: 2.2074\n",
            "Epoch [1/10], Step [370/782], Loss: 2.1764\n",
            "Epoch [1/10], Step [380/782], Loss: 2.2913\n",
            "Epoch [1/10], Step [390/782], Loss: 2.2568\n",
            "Epoch [1/10], Step [400/782], Loss: 2.2336\n",
            "Epoch [1/10], Step [410/782], Loss: 2.2114\n",
            "Epoch [1/10], Step [420/782], Loss: 2.1745\n",
            "Epoch [1/10], Step [430/782], Loss: 2.2192\n",
            "Epoch [1/10], Step [440/782], Loss: 2.2609\n",
            "Epoch [1/10], Step [450/782], Loss: 2.2218\n",
            "Epoch [1/10], Step [460/782], Loss: 2.2581\n",
            "Epoch [1/10], Step [470/782], Loss: 2.2204\n",
            "Epoch [1/10], Step [480/782], Loss: 2.1698\n",
            "Epoch [1/10], Step [490/782], Loss: 2.2239\n",
            "Epoch [1/10], Step [500/782], Loss: 2.2298\n",
            "Epoch [1/10], Step [510/782], Loss: 2.2007\n",
            "Epoch [1/10], Step [520/782], Loss: 2.2106\n",
            "Epoch [1/10], Step [530/782], Loss: 2.2052\n",
            "Epoch [1/10], Step [540/782], Loss: 2.2517\n",
            "Epoch [1/10], Step [550/782], Loss: 2.2832\n",
            "Epoch [1/10], Step [560/782], Loss: 2.2439\n",
            "Epoch [1/10], Step [570/782], Loss: 2.2916\n",
            "Epoch [1/10], Step [580/782], Loss: 2.2276\n",
            "Epoch [1/10], Step [590/782], Loss: 2.1763\n",
            "Epoch [1/10], Step [600/782], Loss: 2.1967\n",
            "Epoch [1/10], Step [610/782], Loss: 2.2156\n",
            "Epoch [1/10], Step [620/782], Loss: 2.2577\n",
            "Epoch [1/10], Step [630/782], Loss: 2.2018\n",
            "Epoch [1/10], Step [640/782], Loss: 2.2022\n",
            "Epoch [1/10], Step [650/782], Loss: 2.1820\n",
            "Epoch [1/10], Step [660/782], Loss: 2.1867\n",
            "Epoch [1/10], Step [670/782], Loss: 2.2063\n",
            "Epoch [1/10], Step [680/782], Loss: 2.2529\n",
            "Epoch [1/10], Step [690/782], Loss: 2.1436\n",
            "Epoch [1/10], Step [700/782], Loss: 2.1751\n",
            "Epoch [1/10], Step [710/782], Loss: 2.2182\n",
            "Epoch [1/10], Step [720/782], Loss: 2.1924\n",
            "Epoch [1/10], Step [730/782], Loss: 2.1521\n",
            "Epoch [1/10], Step [740/782], Loss: 2.1774\n",
            "Epoch [1/10], Step [750/782], Loss: 2.2262\n",
            "Epoch [1/10], Step [760/782], Loss: 2.1957\n",
            "Epoch [1/10], Step [770/782], Loss: 2.1286\n",
            "Epoch [1/10], Step [780/782], Loss: 2.1527\n",
            "Epoch [2/10], Step [10/782], Loss: 2.1921\n",
            "Epoch [2/10], Step [20/782], Loss: 2.2581\n",
            "Epoch [2/10], Step [30/782], Loss: 2.1428\n",
            "Epoch [2/10], Step [40/782], Loss: 2.2027\n",
            "Epoch [2/10], Step [50/782], Loss: 2.2015\n",
            "Epoch [2/10], Step [60/782], Loss: 2.1869\n",
            "Epoch [2/10], Step [70/782], Loss: 2.1620\n",
            "Epoch [2/10], Step [80/782], Loss: 2.2496\n",
            "Epoch [2/10], Step [90/782], Loss: 2.2074\n",
            "Epoch [2/10], Step [100/782], Loss: 2.1448\n",
            "Epoch [2/10], Step [110/782], Loss: 2.2404\n",
            "Epoch [2/10], Step [120/782], Loss: 2.0832\n",
            "Epoch [2/10], Step [130/782], Loss: 2.1467\n",
            "Epoch [2/10], Step [140/782], Loss: 2.1609\n",
            "Epoch [2/10], Step [150/782], Loss: 2.1945\n",
            "Epoch [2/10], Step [160/782], Loss: 2.1299\n",
            "Epoch [2/10], Step [170/782], Loss: 2.1438\n",
            "Epoch [2/10], Step [180/782], Loss: 2.2280\n",
            "Epoch [2/10], Step [190/782], Loss: 2.1354\n",
            "Epoch [2/10], Step [200/782], Loss: 2.1906\n",
            "Epoch [2/10], Step [210/782], Loss: 2.1442\n",
            "Epoch [2/10], Step [220/782], Loss: 2.1598\n",
            "Epoch [2/10], Step [230/782], Loss: 2.1556\n",
            "Epoch [2/10], Step [240/782], Loss: 2.1915\n",
            "Epoch [2/10], Step [250/782], Loss: 2.1236\n",
            "Epoch [2/10], Step [260/782], Loss: 2.1952\n",
            "Epoch [2/10], Step [270/782], Loss: 2.1406\n",
            "Epoch [2/10], Step [280/782], Loss: 2.1734\n",
            "Epoch [2/10], Step [290/782], Loss: 2.1397\n",
            "Epoch [2/10], Step [300/782], Loss: 2.2433\n",
            "Epoch [2/10], Step [310/782], Loss: 2.1959\n",
            "Epoch [2/10], Step [320/782], Loss: 2.2922\n",
            "Epoch [2/10], Step [330/782], Loss: 2.2346\n",
            "Epoch [2/10], Step [340/782], Loss: 2.1643\n",
            "Epoch [2/10], Step [350/782], Loss: 2.1595\n",
            "Epoch [2/10], Step [360/782], Loss: 2.1893\n",
            "Epoch [2/10], Step [370/782], Loss: 2.1610\n",
            "Epoch [2/10], Step [380/782], Loss: 2.2303\n",
            "Epoch [2/10], Step [390/782], Loss: 2.1387\n",
            "Epoch [2/10], Step [400/782], Loss: 2.1181\n",
            "Epoch [2/10], Step [410/782], Loss: 2.2155\n",
            "Epoch [2/10], Step [420/782], Loss: 2.1484\n",
            "Epoch [2/10], Step [430/782], Loss: 2.0657\n",
            "Epoch [2/10], Step [440/782], Loss: 2.1369\n",
            "Epoch [2/10], Step [450/782], Loss: 2.0938\n",
            "Epoch [2/10], Step [460/782], Loss: 2.1145\n",
            "Epoch [2/10], Step [470/782], Loss: 2.0753\n",
            "Epoch [2/10], Step [480/782], Loss: 2.1764\n",
            "Epoch [2/10], Step [490/782], Loss: 2.1770\n",
            "Epoch [2/10], Step [500/782], Loss: 2.2027\n",
            "Epoch [2/10], Step [510/782], Loss: 2.1697\n",
            "Epoch [2/10], Step [520/782], Loss: 2.0805\n",
            "Epoch [2/10], Step [530/782], Loss: 2.1248\n",
            "Epoch [2/10], Step [540/782], Loss: 2.1372\n",
            "Epoch [2/10], Step [550/782], Loss: 2.1468\n",
            "Epoch [2/10], Step [560/782], Loss: 2.1670\n",
            "Epoch [2/10], Step [570/782], Loss: 2.1933\n",
            "Epoch [2/10], Step [580/782], Loss: 2.1434\n",
            "Epoch [2/10], Step [590/782], Loss: 2.1807\n",
            "Epoch [2/10], Step [600/782], Loss: 2.3027\n",
            "Epoch [2/10], Step [610/782], Loss: 2.0940\n",
            "Epoch [2/10], Step [620/782], Loss: 2.2530\n",
            "Epoch [2/10], Step [630/782], Loss: 2.1525\n",
            "Epoch [2/10], Step [640/782], Loss: 2.1264\n",
            "Epoch [2/10], Step [650/782], Loss: 2.0219\n",
            "Epoch [2/10], Step [660/782], Loss: 2.0605\n",
            "Epoch [2/10], Step [670/782], Loss: 2.2255\n",
            "Epoch [2/10], Step [680/782], Loss: 2.0911\n",
            "Epoch [2/10], Step [690/782], Loss: 2.1710\n",
            "Epoch [2/10], Step [700/782], Loss: 2.0807\n",
            "Epoch [2/10], Step [710/782], Loss: 2.1541\n",
            "Epoch [2/10], Step [720/782], Loss: 2.1053\n",
            "Epoch [2/10], Step [730/782], Loss: 2.1058\n",
            "Epoch [2/10], Step [740/782], Loss: 2.1047\n",
            "Epoch [2/10], Step [750/782], Loss: 2.1219\n",
            "Epoch [2/10], Step [760/782], Loss: 2.1088\n",
            "Epoch [2/10], Step [770/782], Loss: 2.1351\n",
            "Epoch [2/10], Step [780/782], Loss: 2.1444\n",
            "Epoch [3/10], Step [10/782], Loss: 2.1712\n",
            "Epoch [3/10], Step [20/782], Loss: 2.0914\n",
            "Epoch [3/10], Step [30/782], Loss: 2.0611\n",
            "Epoch [3/10], Step [40/782], Loss: 2.1077\n",
            "Epoch [3/10], Step [50/782], Loss: 2.0742\n",
            "Epoch [3/10], Step [60/782], Loss: 2.1435\n",
            "Epoch [3/10], Step [70/782], Loss: 2.0680\n",
            "Epoch [3/10], Step [80/782], Loss: 2.1187\n",
            "Epoch [3/10], Step [90/782], Loss: 2.1883\n",
            "Epoch [3/10], Step [100/782], Loss: 2.1292\n",
            "Epoch [3/10], Step [110/782], Loss: 1.9803\n",
            "Epoch [3/10], Step [120/782], Loss: 2.0728\n",
            "Epoch [3/10], Step [130/782], Loss: 2.1080\n",
            "Epoch [3/10], Step [140/782], Loss: 2.0788\n",
            "Epoch [3/10], Step [150/782], Loss: 2.0959\n",
            "Epoch [3/10], Step [160/782], Loss: 2.1481\n",
            "Epoch [3/10], Step [170/782], Loss: 2.0668\n",
            "Epoch [3/10], Step [180/782], Loss: 2.0890\n",
            "Epoch [3/10], Step [190/782], Loss: 2.1298\n",
            "Epoch [3/10], Step [200/782], Loss: 2.1794\n",
            "Epoch [3/10], Step [210/782], Loss: 2.1500\n",
            "Epoch [3/10], Step [220/782], Loss: 2.1866\n",
            "Epoch [3/10], Step [230/782], Loss: 2.1530\n",
            "Epoch [3/10], Step [240/782], Loss: 2.1178\n",
            "Epoch [3/10], Step [250/782], Loss: 2.0400\n",
            "Epoch [3/10], Step [260/782], Loss: 2.1199\n",
            "Epoch [3/10], Step [270/782], Loss: 2.1006\n",
            "Epoch [3/10], Step [280/782], Loss: 2.1057\n",
            "Epoch [3/10], Step [290/782], Loss: 2.0089\n",
            "Epoch [3/10], Step [300/782], Loss: 2.1361\n",
            "Epoch [3/10], Step [310/782], Loss: 2.1695\n",
            "Epoch [3/10], Step [320/782], Loss: 2.0341\n",
            "Epoch [3/10], Step [330/782], Loss: 2.1640\n",
            "Epoch [3/10], Step [340/782], Loss: 2.2030\n",
            "Epoch [3/10], Step [350/782], Loss: 2.0793\n",
            "Epoch [3/10], Step [360/782], Loss: 2.1157\n",
            "Epoch [3/10], Step [370/782], Loss: 2.0871\n",
            "Epoch [3/10], Step [380/782], Loss: 2.1245\n",
            "Epoch [3/10], Step [390/782], Loss: 2.0761\n",
            "Epoch [3/10], Step [400/782], Loss: 2.0813\n",
            "Epoch [3/10], Step [410/782], Loss: 2.1095\n",
            "Epoch [3/10], Step [420/782], Loss: 2.0752\n",
            "Epoch [3/10], Step [430/782], Loss: 2.1181\n",
            "Epoch [3/10], Step [440/782], Loss: 2.1322\n",
            "Epoch [3/10], Step [450/782], Loss: 2.0658\n",
            "Epoch [3/10], Step [460/782], Loss: 2.0926\n",
            "Epoch [3/10], Step [470/782], Loss: 2.1759\n",
            "Epoch [3/10], Step [480/782], Loss: 2.2430\n",
            "Epoch [3/10], Step [490/782], Loss: 2.1141\n",
            "Epoch [3/10], Step [500/782], Loss: 2.1079\n",
            "Epoch [3/10], Step [510/782], Loss: 2.0608\n",
            "Epoch [3/10], Step [520/782], Loss: 2.1181\n",
            "Epoch [3/10], Step [530/782], Loss: 2.1485\n",
            "Epoch [3/10], Step [540/782], Loss: 2.2130\n",
            "Epoch [3/10], Step [550/782], Loss: 2.1193\n",
            "Epoch [3/10], Step [560/782], Loss: 2.1012\n",
            "Epoch [3/10], Step [570/782], Loss: 2.0708\n",
            "Epoch [3/10], Step [580/782], Loss: 2.0755\n",
            "Epoch [3/10], Step [590/782], Loss: 2.0614\n",
            "Epoch [3/10], Step [600/782], Loss: 1.9901\n",
            "Epoch [3/10], Step [610/782], Loss: 2.1174\n",
            "Epoch [3/10], Step [620/782], Loss: 2.0706\n",
            "Epoch [3/10], Step [630/782], Loss: 2.1687\n",
            "Epoch [3/10], Step [640/782], Loss: 2.1973\n",
            "Epoch [3/10], Step [650/782], Loss: 2.1278\n",
            "Epoch [3/10], Step [660/782], Loss: 2.1246\n",
            "Epoch [3/10], Step [670/782], Loss: 2.1652\n",
            "Epoch [3/10], Step [680/782], Loss: 2.1293\n",
            "Epoch [3/10], Step [690/782], Loss: 2.1121\n",
            "Epoch [3/10], Step [700/782], Loss: 2.0872\n",
            "Epoch [3/10], Step [710/782], Loss: 2.1230\n",
            "Epoch [3/10], Step [720/782], Loss: 2.1222\n",
            "Epoch [3/10], Step [730/782], Loss: 2.1570\n",
            "Epoch [3/10], Step [740/782], Loss: 2.1985\n",
            "Epoch [3/10], Step [750/782], Loss: 2.1697\n",
            "Epoch [3/10], Step [760/782], Loss: 2.1365\n",
            "Epoch [3/10], Step [770/782], Loss: 2.0496\n",
            "Epoch [3/10], Step [780/782], Loss: 2.0240\n",
            "Epoch [4/10], Step [10/782], Loss: 2.0803\n",
            "Epoch [4/10], Step [20/782], Loss: 2.0749\n",
            "Epoch [4/10], Step [30/782], Loss: 2.1456\n",
            "Epoch [4/10], Step [40/782], Loss: 2.0021\n",
            "Epoch [4/10], Step [50/782], Loss: 2.1961\n",
            "Epoch [4/10], Step [60/782], Loss: 2.1158\n",
            "Epoch [4/10], Step [70/782], Loss: 2.0967\n",
            "Epoch [4/10], Step [80/782], Loss: 2.1000\n",
            "Epoch [4/10], Step [90/782], Loss: 2.1536\n",
            "Epoch [4/10], Step [100/782], Loss: 2.1220\n",
            "Epoch [4/10], Step [110/782], Loss: 2.1326\n",
            "Epoch [4/10], Step [120/782], Loss: 2.0914\n",
            "Epoch [4/10], Step [130/782], Loss: 2.0568\n",
            "Epoch [4/10], Step [140/782], Loss: 2.1177\n",
            "Epoch [4/10], Step [150/782], Loss: 2.1373\n",
            "Epoch [4/10], Step [160/782], Loss: 2.1897\n",
            "Epoch [4/10], Step [170/782], Loss: 2.1802\n",
            "Epoch [4/10], Step [180/782], Loss: 2.0694\n",
            "Epoch [4/10], Step [190/782], Loss: 2.0534\n",
            "Epoch [4/10], Step [200/782], Loss: 2.1200\n",
            "Epoch [4/10], Step [210/782], Loss: 2.1868\n",
            "Epoch [4/10], Step [220/782], Loss: 2.1064\n",
            "Epoch [4/10], Step [230/782], Loss: 2.0979\n",
            "Epoch [4/10], Step [240/782], Loss: 1.9654\n",
            "Epoch [4/10], Step [250/782], Loss: 2.2196\n",
            "Epoch [4/10], Step [260/782], Loss: 2.0175\n",
            "Epoch [4/10], Step [270/782], Loss: 2.1103\n",
            "Epoch [4/10], Step [280/782], Loss: 2.0691\n",
            "Epoch [4/10], Step [290/782], Loss: 2.0927\n",
            "Epoch [4/10], Step [300/782], Loss: 2.1513\n",
            "Epoch [4/10], Step [310/782], Loss: 2.0727\n",
            "Epoch [4/10], Step [320/782], Loss: 2.0799\n",
            "Epoch [4/10], Step [330/782], Loss: 2.1359\n",
            "Epoch [4/10], Step [340/782], Loss: 2.0703\n",
            "Epoch [4/10], Step [350/782], Loss: 2.0960\n",
            "Epoch [4/10], Step [360/782], Loss: 2.1263\n",
            "Epoch [4/10], Step [370/782], Loss: 2.1360\n",
            "Epoch [4/10], Step [380/782], Loss: 2.1538\n",
            "Epoch [4/10], Step [390/782], Loss: 2.0468\n",
            "Epoch [4/10], Step [400/782], Loss: 2.0717\n",
            "Epoch [4/10], Step [410/782], Loss: 2.1513\n",
            "Epoch [4/10], Step [420/782], Loss: 2.1584\n",
            "Epoch [4/10], Step [430/782], Loss: 2.1036\n",
            "Epoch [4/10], Step [440/782], Loss: 2.1147\n",
            "Epoch [4/10], Step [450/782], Loss: 2.0311\n",
            "Epoch [4/10], Step [460/782], Loss: 2.1825\n",
            "Epoch [4/10], Step [470/782], Loss: 2.0781\n",
            "Epoch [4/10], Step [480/782], Loss: 2.1064\n",
            "Epoch [4/10], Step [490/782], Loss: 2.1563\n",
            "Epoch [4/10], Step [500/782], Loss: 2.1370\n",
            "Epoch [4/10], Step [510/782], Loss: 2.1481\n",
            "Epoch [4/10], Step [520/782], Loss: 2.1383\n",
            "Epoch [4/10], Step [530/782], Loss: 2.0541\n",
            "Epoch [4/10], Step [540/782], Loss: 2.1155\n",
            "Epoch [4/10], Step [550/782], Loss: 2.0716\n",
            "Epoch [4/10], Step [560/782], Loss: 2.1489\n",
            "Epoch [4/10], Step [570/782], Loss: 2.1504\n",
            "Epoch [4/10], Step [580/782], Loss: 2.0911\n",
            "Epoch [4/10], Step [590/782], Loss: 2.0554\n",
            "Epoch [4/10], Step [600/782], Loss: 2.1628\n",
            "Epoch [4/10], Step [610/782], Loss: 2.1337\n",
            "Epoch [4/10], Step [620/782], Loss: 2.0418\n",
            "Epoch [4/10], Step [630/782], Loss: 2.1667\n",
            "Epoch [4/10], Step [640/782], Loss: 2.1947\n",
            "Epoch [4/10], Step [650/782], Loss: 2.0167\n",
            "Epoch [4/10], Step [660/782], Loss: 2.0722\n",
            "Epoch [4/10], Step [670/782], Loss: 2.1269\n",
            "Epoch [4/10], Step [680/782], Loss: 2.0813\n",
            "Epoch [4/10], Step [690/782], Loss: 2.1168\n",
            "Epoch [4/10], Step [700/782], Loss: 2.1786\n",
            "Epoch [4/10], Step [710/782], Loss: 2.1866\n",
            "Epoch [4/10], Step [720/782], Loss: 2.0389\n",
            "Epoch [4/10], Step [730/782], Loss: 2.1142\n",
            "Epoch [4/10], Step [740/782], Loss: 2.1317\n",
            "Epoch [4/10], Step [750/782], Loss: 2.0784\n",
            "Epoch [4/10], Step [760/782], Loss: 2.1244\n",
            "Epoch [4/10], Step [770/782], Loss: 2.1389\n",
            "Epoch [4/10], Step [780/782], Loss: 2.0390\n",
            "Epoch [5/10], Step [10/782], Loss: 2.1891\n",
            "Epoch [5/10], Step [20/782], Loss: 2.1051\n",
            "Epoch [5/10], Step [30/782], Loss: 2.0689\n",
            "Epoch [5/10], Step [40/782], Loss: 2.1650\n",
            "Epoch [5/10], Step [50/782], Loss: 2.0987\n",
            "Epoch [5/10], Step [60/782], Loss: 2.0586\n",
            "Epoch [5/10], Step [70/782], Loss: 2.0590\n",
            "Epoch [5/10], Step [80/782], Loss: 2.2343\n",
            "Epoch [5/10], Step [90/782], Loss: 2.1029\n",
            "Epoch [5/10], Step [100/782], Loss: 2.0429\n",
            "Epoch [5/10], Step [110/782], Loss: 2.0896\n",
            "Epoch [5/10], Step [120/782], Loss: 2.0906\n",
            "Epoch [5/10], Step [130/782], Loss: 2.0826\n",
            "Epoch [5/10], Step [140/782], Loss: 2.0636\n",
            "Epoch [5/10], Step [150/782], Loss: 1.9990\n",
            "Epoch [5/10], Step [160/782], Loss: 2.0693\n",
            "Epoch [5/10], Step [170/782], Loss: 2.0871\n",
            "Epoch [5/10], Step [180/782], Loss: 2.1370\n",
            "Epoch [5/10], Step [190/782], Loss: 2.2022\n",
            "Epoch [5/10], Step [200/782], Loss: 2.0801\n",
            "Epoch [5/10], Step [210/782], Loss: 1.9823\n",
            "Epoch [5/10], Step [220/782], Loss: 2.0813\n",
            "Epoch [5/10], Step [230/782], Loss: 2.1604\n",
            "Epoch [5/10], Step [240/782], Loss: 2.0803\n",
            "Epoch [5/10], Step [250/782], Loss: 2.1209\n",
            "Epoch [5/10], Step [260/782], Loss: 2.0547\n",
            "Epoch [5/10], Step [270/782], Loss: 2.1234\n",
            "Epoch [5/10], Step [280/782], Loss: 2.1004\n",
            "Epoch [5/10], Step [290/782], Loss: 2.1621\n",
            "Epoch [5/10], Step [300/782], Loss: 2.1991\n",
            "Epoch [5/10], Step [310/782], Loss: 2.0837\n",
            "Epoch [5/10], Step [320/782], Loss: 2.0627\n",
            "Epoch [5/10], Step [330/782], Loss: 2.0463\n",
            "Epoch [5/10], Step [340/782], Loss: 2.0712\n",
            "Epoch [5/10], Step [350/782], Loss: 2.0365\n",
            "Epoch [5/10], Step [360/782], Loss: 2.1259\n",
            "Epoch [5/10], Step [370/782], Loss: 2.1516\n",
            "Epoch [5/10], Step [380/782], Loss: 2.0271\n",
            "Epoch [5/10], Step [390/782], Loss: 2.1123\n",
            "Epoch [5/10], Step [400/782], Loss: 2.1206\n"
          ]
        }
      ]
    }
  ]
}